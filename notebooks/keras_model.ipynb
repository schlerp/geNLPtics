{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import keras\n",
    "import keras.layers\n",
    "import keras.utils.all_utils\n",
    "import keras.callbacks\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-19 12:55:01.572199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-19 12:55:01.577433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-19 12:55:01.577554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17959\n"
     ]
    }
   ],
   "source": [
    "def parse_fasta_dataset(file_path: str = \"../data/LTP_09_2021_compressed.fasta\", max_seqs:int = False) -> List[Dict[str, str]]:\n",
    "    dataset = []\n",
    "    current_idx = 0\n",
    "    current_meta = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if line[0] == \">\":\n",
    "                if current_meta != {}:\n",
    "                    current_meta[\"sequence\"] = current_meta[\"sequence\"].strip()\n",
    "                    dataset.append(current_meta)\n",
    "                line_list = line.replace(\"\\n\", \"\").replace(\">\", \"\").split(\"\\t\")\n",
    "                if len(line_list) < 2:\n",
    "                    current_meta  = {key: line_list[idx] for idx, key in enumerate([\"name\"])}\n",
    "                elif len(line_list) < 3:\n",
    "                    current_meta  = {key: line_list[idx] for idx, key in enumerate([\"id\", \"name\"])}\n",
    "                else:\n",
    "                    current_meta  = {key: line_list[idx] for idx, key in enumerate([\"id\", \"name\", \"tags\"])}\n",
    "                    current_meta[\"tags\"] = current_meta[\"tags\"].split(\";\")\n",
    "                current_meta[\"sequence\"] = \"\"\n",
    "                current_idx += 1\n",
    "                if current_idx > max_seqs and max_seqs>=1:\n",
    "                    break\n",
    "            else:\n",
    "                current_meta[\"sequence\"] += line.replace(\"\\n\", \" \")\n",
    "    return dataset\n",
    "\n",
    "dataset = parse_fasta_dataset(max_seqs=-1)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'AB681979',\n",
       " 'name': 'Trabulsiella guamensis',\n",
       " 'tags': ['Bacteria',\n",
       "  'Proteobacteria',\n",
       "  'Gammaproteobacteria',\n",
       "  'Enterobacterales',\n",
       "  'Enterobacteriaceae',\n",
       "  'Trabulsiella'],\n",
       " 'sequence': 'AUUGAACGCU GGCGGCAGGC CUAACACAUG CAAGUCGAGC GGCAGCGGGG GAAAGCUUGC UUUCCCGCCG GCGAGCGGCG GACGGGUGAG UAAUGUCUGG GAAACUGCCU GAUGGAGGGG GAUAACUACU GGAAACGGUA GCUAAUACCG CAUAACGUCU UCGGACCAAA GUGGGGGACC UUCGGGCCUC AUGCCAUCAG AUGUGCCCAG AUGGGAUUAG CUAGUAGGUG GGGUAACGGC UCACCUAGGC GACGAUCCCU AGCUGGUCUG AGAGGAUGAC CAGCCACACU GGAACUGAGA CACGGUCCAG ACUCCUACGG GAGGCAGCAG UGGGGAAUAU UGCACAAUGG GCGCAAGCCU GAUGCAGCCA UGCCGCGUGU AUGAAGAAGG CCUUCGGGUU GUAAAGUACU UUCAGCGGGG AGGAAGGUGU UGUGGUUAAU AACCAGAGCA AUUGACGUUA CCCGCAGAAG AAGCACCGGC UAACUCCGUG CCAGCAGCCG CGGUAAUACG GAGGGUGCAA GCGUUAAUCG GAAUUACUGG GCGUAAAGCG CACGCAGGCG GUCUGUCAAG UCGGAUGUGA AAUCCCCGGG CUCAACCUGG GAACUGCAUC CGAAACUGGC AGGCUUGAGU CUUGUAGAGG GGGGUAGAAU UCCAGGUGUA GCGGUGAAAU GCGUAGAGAU CUGGAGGAAU ACCGGUGGCG AAGGCGGCCC CCUGGACAAA GACUGACGCU CAGGUGCGAA AGCGUGGGGA GCAAACAGGA UUAGAUACCC UGGUAGUCCA CGCCGUAAAC GAUGUCGACU UGGAGGUUGU GCCCUUGAGG CGUGGCUUCC GGAGCUAACG CGUUAAGUCG ACCGCCUGGG GAGUACGGCC GCAAGGUUAA AACUCAAAUG AAUUGACGGG GGCCCGCACA AGCGGUGGAG CAUGUGGUUU AAUUCGAUGC AACGCGAAGA ACCUUACCUG GUCUUGACAU CCACAGAACC CUGUAGAGAU ACGGGGGUGC CUUCGGGAAC UGUGAGACAG GUGCUGCAUG GCUGUCGUCA GCUCGUGUUG UGAAAUGUUG GGUUAAGUCC CGCAACGAGC GCAACCCUUA UCCUUUGUUG CCAGCGGUCC GGCCGGGAAC UCAAAGGAGA CUGCCAGUGA UAAACUGGAG GAAGGUGGGG AUGACGUCAA GUCAUCAUGG CCCUUACGAC CAGGGCUACA CACGUGCUAC AAUGGCAUAU ACAAAGAGAA GCGACCUCGC GAGAGCAAGC GGACCUCAUA AAGUAUGUCG UAGUCCGGAU UGGAGUCUGC AACUCGACUC CAUGAAGUCG GAAUCGCUAG UAAUCGUGGA UCAGAAUGCC ACGGUGAAUA CGUUCCCGGG CCUUGUACAC ACCGCCCGUC ACACCAUGGG AGUGGGUUGC AAAAGAAGUA GGUAGCUUAA CCUUCGGGAG GGCGCUUACC ACUUUGUGAU UCAUGACUGG GGUGAAG'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_seq(seq: str, chunk_len: int = 10):\n",
    "    ret_list = []\n",
    "    chunk = \"\"\n",
    "    for char in seq:\n",
    "        chunk += char\n",
    "        if len(chunk) % chunk_len ==0:\n",
    "            ret_list.append(chunk)\n",
    "            chunk=\"\"\n",
    "    if chunk != '':\n",
    "        ret_list.append(\"{}{}\".format(chunk, \"_\"*(4-len(chunk))))\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for kmer_seq in [chunk_seq(x[\"sequence\"].replace(\" \", \"\")) for x in dataset]:\n",
    "    temp.extend(kmer_seq)\n",
    "temp = set(temp)\n",
    "encode_dict = {value: idx+1 for idx, value in enumerate(temp)}\n",
    "# print(encode_dict)\n",
    "X = np.array([np.pad(np.array([encode_dict[z] for z in chunk_seq(x[\"sequence\"].replace(\" \", \"\"))]), (0, 3000))[:1800] for x in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17959, 1800)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {value for x in dataset for value in x[\"tags\"][:3]}\n",
    "label_encode_dict = {value: idx+1 for idx, value in enumerate(temp)}\n",
    "Y_category = keras.utils.all_utils.to_categorical(np.array([[label_encode_dict[z] for z in x[\"tags\"][:3]] for x in dataset]))\n",
    "Y_category = np.array([np.sum(x, 0) for x in Y_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_next = np.array([np.pad(np.array([0.0, 0.0, *[encode_dict[z] for z in chunk_seq(x[\"sequence\"].replace(\" \", \"\"))]]), (0, 3000))[:1500] for x in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_kmer(seq: List, chunk_len: int = 5):\n",
    "    ret_list = []\n",
    "    chunk = []\n",
    "    for char in seq:\n",
    "        chunk.append(char)\n",
    "        if len(chunk) % chunk_len ==0:\n",
    "            ret_list.append(chunk)\n",
    "            chunk=[]\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sg = np.array([[z for z in chunk_kmer(x)] for x in Y_next])\n",
    "Y_sg.shape\n",
    "Y_sg = np.array([[[*z[:2], *z[3:]] for z in x] for x in Y_sg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61830\n",
      "[     0.      0. 243212. 137227.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][0])\n",
    "print(Y_sg[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_block(\n",
    "    x, \n",
    "    filters=16, \n",
    "    kernel=3, \n",
    "    stride=1, \n",
    "    ratio=2, \n",
    "    act=\"elu\", \n",
    "    padding=\"same\"\n",
    "):\n",
    "    # dimensionality reduction\n",
    "    x = keras.layers.Conv1D(\n",
    "        filters // ratio,\n",
    "        kernel_size=1,\n",
    "        strides=stride,\n",
    "        padding=padding,\n",
    "    )(x)\n",
    "\n",
    "    # convolution\n",
    "    x = keras.layers.Conv1D(\n",
    "        filters,\n",
    "        kernel_size=kernel,\n",
    "        strides=stride,\n",
    "        padding=padding,\n",
    "    )(x)\n",
    "\n",
    "    # batch norm\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    # activation\n",
    "    x = keras.layers.Activation(act)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(\n",
    "    input_layer, \n",
    "    width: int = 128, \n",
    "    dropout: float = 0.3, \n",
    "    attention_filter: int = 8, \n",
    "    attention_dilation:int = 1, \n",
    "    attention_activation: str = \"softmax\"\n",
    "):\n",
    "    # RNN layer\n",
    "    rnn_layer = keras.layers.CuDNNGRU(\n",
    "        width, \n",
    "    )(input_layer)\n",
    "    \n",
    "    attention_layer = conv_bn_block(input_layer, filters=width, kernel=64)\n",
    "    \n",
    "    multiply_layer = keras.layers.Multiply()([rnn_layer, attention_layer])\n",
    "    \n",
    "    output_layer = keras.layers.Dense(width, activation=\"linear\")(multiply_layer)\n",
    "    \n",
    "    # dropout\n",
    "    if dropout:\n",
    "        output_layer = keras.layers.Dropout(dropout)(output_layer)\n",
    "    \n",
    "    return keras.layers.Activation(activation=attention_activation)(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape: Tuple[int], \n",
    "    output_shape: Tuple[int], \n",
    "    embed_size: int, \n",
    "    vocab_size: int, \n",
    "    rnn_size: int = 32\n",
    "):\n",
    "    # model input\n",
    "    model_input = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # embedding layer\n",
    "    embedding = keras.layers.Embedding(vocab_size, embed_size)(model_input)\n",
    "    \n",
    "    rnn_layer = attention_layer(embedding)\n",
    "    \n",
    "    # pooling layer\n",
    "    pooling_layer = keras.layers.GlobalAveragePooling1D()(rnn_layer)\n",
    "    \n",
    "    # model output\n",
    "    model_output = keras.layers.Dense(\n",
    "        output_shape[0] * output_shape[1], \n",
    "        activation=\"relu\", \n",
    "        kernel_regularizer=\"l2\"\n",
    "    )(pooling_layer)\n",
    "    \n",
    "    model_output = keras.layers.Reshape([*output_shape])(model_output)\n",
    "\n",
    "    return keras.Model(inputs=[model_input], outputs=[model_output]), keras.Model(inputs=[model_input], outputs=[embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 1800)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 1800, 8)      1990072     ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 1800, 64)     576         ['embedding_11[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 1800, 128)    524416      ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 1800, 128)   512         ['conv1d_13[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " cu_dnngru_11 (CuDNNGRU)        (None, 128)          52992       ['embedding_11[0][0]']           \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 1800, 128)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " multiply_6 (Multiply)          (None, 1800, 128)    0           ['cu_dnngru_11[0][0]',           \n",
      "                                                                  'activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 1800, 128)    16512       ['multiply_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 1800, 128)    0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 1800, 128)    0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6 (Gl  (None, 128)         0           ['activation_13[0][0]']          \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1200)         154800      ['global_average_pooling1d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 300, 4)       0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,739,880\n",
      "Trainable params: 2,739,624\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "113/113 [==============================] - 20s 171ms/step - loss: 71960.3047 - val_loss: 36405.7695\n",
      "Epoch 2/1000\n",
      "113/113 [==============================] - 19s 170ms/step - loss: 28114.8496 - val_loss: 19458.4434\n",
      "Epoch 3/1000\n",
      "113/113 [==============================] - 19s 170ms/step - loss: 13667.1475 - val_loss: 7691.4014\n",
      "Epoch 4/1000\n",
      "113/113 [==============================] - 19s 170ms/step - loss: 1355.5793 - val_loss: -7055.4883\n",
      "Epoch 5/1000\n",
      "113/113 [==============================] - 19s 170ms/step - loss: -66670.3906 - val_loss: -101850.7891\n",
      "Epoch 6/1000\n",
      "113/113 [==============================] - 19s 170ms/step - loss: -134495.1562 - val_loss: -142144.4062\n",
      "Epoch 7/1000\n",
      "113/113 [==============================] - 19s 170ms/step - loss: -165804.7344 - val_loss: -159702.6406\n",
      "Epoch 8/1000\n",
      "113/113 [==============================] - 19s 170ms/step - loss: -167552.0938 - val_loss: -18015.8145\n",
      "Epoch 9/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167569.5781 - val_loss: -161819.7344\n",
      "Epoch 10/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167568.6562 - val_loss: -25948.4629\n",
      "Epoch 11/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167571.9062 - val_loss: -12452.9941\n",
      "Epoch 12/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167574.7500 - val_loss: -164693.9844\n",
      "Epoch 13/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167578.1562 - val_loss: -9586.5410\n",
      "Epoch 14/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167575.5469 - val_loss: -66478.2734\n",
      "Epoch 15/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167579.6875 - val_loss: -164707.7656\n",
      "Epoch 16/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167582.7812 - val_loss: -164708.4844\n",
      "Epoch 17/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.2031 - val_loss: -6226.5317\n",
      "Epoch 18/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.6562 - val_loss: -5450.2969\n",
      "Epoch 19/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167584.5156 - val_loss: -164713.5000\n",
      "Epoch 20/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.6562 - val_loss: -6272.3066\n",
      "Epoch 21/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.8438 - val_loss: -164713.6719\n",
      "Epoch 22/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167583.3906 - val_loss: -164713.7031\n",
      "Epoch 23/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.4844 - val_loss: -2807.4539\n",
      "Epoch 24/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.3438 - val_loss: -164713.7969\n",
      "Epoch 25/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.1094 - val_loss: -2918.9575\n",
      "Epoch 26/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167582.5781 - val_loss: -164713.8906\n",
      "Epoch 27/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.4375 - val_loss: -164713.9375\n",
      "Epoch 28/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167583.2656 - val_loss: -164714.0469\n",
      "Epoch 29/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.6250 - val_loss: -164714.2656\n",
      "Epoch 30/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.5156 - val_loss: -164714.2812\n",
      "Epoch 31/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.3281 - val_loss: -113.4727\n",
      "Epoch 32/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.2969 - val_loss: -164714.3750\n",
      "Epoch 33/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.7500 - val_loss: -164714.4219\n",
      "Epoch 34/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.0625 - val_loss: -164714.9688\n",
      "Epoch 35/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167580.1875 - val_loss: -164714.9688\n",
      "Epoch 36/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.6562 - val_loss: 1248.8887\n",
      "Epoch 37/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167592.9531 - val_loss: -164715.0469\n",
      "Epoch 38/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167586.5938 - val_loss: -164715.0781\n",
      "Epoch 39/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.8438 - val_loss: -164715.0781\n",
      "Epoch 40/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167581.3906 - val_loss: -164715.1250\n",
      "Epoch 41/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.2031 - val_loss: -164715.1719\n",
      "Epoch 42/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167581.8594 - val_loss: -164715.1875\n",
      "Epoch 43/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.4375 - val_loss: -164715.1875\n",
      "Epoch 44/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.4531 - val_loss: -12834.1592\n",
      "Epoch 45/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.5000 - val_loss: -164715.2500\n",
      "Epoch 46/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.8906 - val_loss: -164715.2656\n",
      "Epoch 47/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.1406 - val_loss: -164715.2656\n",
      "Epoch 48/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167586.8438 - val_loss: -164715.2812\n",
      "Epoch 49/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.2344 - val_loss: -164715.3125\n",
      "Epoch 50/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167586.8750 - val_loss: -164695.6250\n",
      "Epoch 51/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167581.3906 - val_loss: -164715.3438\n",
      "Epoch 52/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.9062 - val_loss: -164715.3438\n",
      "Epoch 53/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167578.2500 - val_loss: -164715.3906\n",
      "Epoch 54/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167592.9219 - val_loss: -164715.4219\n",
      "Epoch 55/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167576.9219 - val_loss: -6938.1890\n",
      "Epoch 56/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.7031 - val_loss: -8019.5991\n",
      "Epoch 57/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167586.9375 - val_loss: -164715.4375\n",
      "Epoch 58/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.0156 - val_loss: -164715.4531\n",
      "Epoch 59/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.2812 - val_loss: -11616.0781\n",
      "Epoch 60/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.4688 - val_loss: -354.8576\n",
      "Epoch 61/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.2812 - val_loss: -164715.5625\n",
      "Epoch 62/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.2812 - val_loss: -164715.5781\n",
      "Epoch 63/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.7656 - val_loss: -164715.5938\n",
      "Epoch 64/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167411.6562 - val_loss: -131351.5625\n",
      "Epoch 65/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -165117.1719 - val_loss: -121474.7578\n",
      "Epoch 66/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -166399.7031 - val_loss: -162259.6875\n",
      "Epoch 67/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167486.7031 - val_loss: -164698.9219\n",
      "Epoch 68/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167574.3750 - val_loss: -164706.5781\n",
      "Epoch 69/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167567.0469 - val_loss: -164697.1875\n",
      "Epoch 70/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167566.2031 - val_loss: -164710.8281\n",
      "Epoch 71/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.0469 - val_loss: -164714.1875\n",
      "Epoch 72/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.4688 - val_loss: -73161.8203\n",
      "Epoch 73/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167574.9062 - val_loss: -164715.0469\n",
      "Epoch 74/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167582.6719 - val_loss: -164715.1562\n",
      "Epoch 75/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.1094 - val_loss: -164715.6406\n",
      "Epoch 76/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167574.7656 - val_loss: -164715.6562\n",
      "Epoch 77/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167583.7031 - val_loss: -164715.6719\n",
      "Epoch 78/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167580.5469 - val_loss: -52803.6172\n",
      "Epoch 79/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.2812 - val_loss: -164715.7188\n",
      "Epoch 80/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167573.9375 - val_loss: -164715.7188\n",
      "Epoch 81/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.4062 - val_loss: -164715.7344\n",
      "Epoch 82/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167573.2188 - val_loss: -164715.7188\n",
      "Epoch 83/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.2969 - val_loss: -164715.7344\n",
      "Epoch 84/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.4219 - val_loss: -164715.7344\n",
      "Epoch 85/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.6406 - val_loss: -164715.7812\n",
      "Epoch 86/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.4688 - val_loss: -164715.7969\n",
      "Epoch 87/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.7344 - val_loss: -60955.6328\n",
      "Epoch 88/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167592.9219 - val_loss: -164715.7969\n",
      "Epoch 89/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.1250 - val_loss: -164715.8125\n",
      "Epoch 90/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.1406 - val_loss: -164715.8125\n",
      "Epoch 91/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.0469 - val_loss: -164715.8125\n",
      "Epoch 92/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.9062 - val_loss: -164715.8281\n",
      "Epoch 93/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.5938 - val_loss: -164715.8438\n",
      "Epoch 94/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.5469 - val_loss: -164715.8594\n",
      "Epoch 95/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.2656 - val_loss: -164715.8438\n",
      "Epoch 96/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167586.4688 - val_loss: -164715.8594\n",
      "Epoch 97/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.4375 - val_loss: -164715.8594\n",
      "Epoch 98/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.7812 - val_loss: -164715.8594\n",
      "Epoch 99/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.4688 - val_loss: -164715.8906\n",
      "Epoch 100/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.5000 - val_loss: -164715.9219\n",
      "Epoch 101/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.4375 - val_loss: -164715.9219\n",
      "Epoch 102/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.2344 - val_loss: -164715.9219\n",
      "Epoch 103/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.3125 - val_loss: -164715.9219\n",
      "Epoch 104/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.6094 - val_loss: -164715.9375\n",
      "Epoch 105/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.7031 - val_loss: -164715.9531\n",
      "Epoch 106/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.9219 - val_loss: -164715.9531\n",
      "Epoch 107/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167592.7656 - val_loss: -164715.9531\n",
      "Epoch 108/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167584.0000 - val_loss: -164715.9531\n",
      "Epoch 109/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167598.3438 - val_loss: -164715.9844\n",
      "Epoch 110/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.4531 - val_loss: -164715.9844\n",
      "Epoch 111/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.2344 - val_loss: -164716.0156\n",
      "Epoch 112/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167579.1875 - val_loss: -164715.9844\n",
      "Epoch 113/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167595.0156 - val_loss: -164716.0156\n",
      "Epoch 114/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.9688 - val_loss: -164716.0312\n",
      "Epoch 115/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.6094 - val_loss: -164716.0469\n",
      "Epoch 116/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.2031 - val_loss: -164716.0781\n",
      "Epoch 117/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.4219 - val_loss: -164716.0938\n",
      "Epoch 118/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.9062 - val_loss: -164716.1562\n",
      "Epoch 119/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.1562 - val_loss: -164716.2188\n",
      "Epoch 120/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.9531 - val_loss: -164716.9219\n",
      "Epoch 121/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.2188 - val_loss: -164716.0781\n",
      "Epoch 122/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.1562 - val_loss: -164716.0938\n",
      "Epoch 123/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.9219 - val_loss: -164716.1719\n",
      "Epoch 124/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.0781 - val_loss: -164716.9219\n",
      "Epoch 125/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.5781 - val_loss: -164716.9219\n",
      "Epoch 126/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167583.6406 - val_loss: -164716.9219\n",
      "Epoch 127/1000\n",
      "113/113 [==============================] - 19s 172ms/step - loss: -167588.7656 - val_loss: -164716.9219\n",
      "Epoch 128/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167584.8906 - val_loss: -164716.9219\n",
      "Epoch 129/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167592.7812 - val_loss: -164716.9219\n",
      "Epoch 130/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.4062 - val_loss: -164716.9219\n",
      "Epoch 131/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.3750 - val_loss: -164716.9219\n",
      "Epoch 132/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.3906 - val_loss: -164716.9375\n",
      "Epoch 133/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.1719 - val_loss: -164716.9375\n",
      "Epoch 134/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.5156 - val_loss: -164716.9375\n",
      "Epoch 135/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167596.0781 - val_loss: -164716.9375\n",
      "Epoch 136/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167586.0000 - val_loss: -164716.9375\n",
      "Epoch 137/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.5781 - val_loss: -164716.9375\n",
      "Epoch 138/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.5000 - val_loss: -164716.9375\n",
      "Epoch 139/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.6406 - val_loss: -164716.9375\n",
      "Epoch 140/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.4531 - val_loss: -164716.9375\n",
      "Epoch 141/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167593.8594 - val_loss: -164716.9375\n",
      "Epoch 142/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.0156 - val_loss: -164716.9375\n",
      "Epoch 143/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167592.9062 - val_loss: -164716.9375\n",
      "Epoch 144/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167595.3125 - val_loss: -164716.3594\n",
      "Epoch 145/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.0938 - val_loss: -164716.3906\n",
      "Epoch 146/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.4844 - val_loss: -164716.4062\n",
      "Epoch 147/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.2812 - val_loss: -164716.4844\n",
      "Epoch 148/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.0000 - val_loss: -164716.9375\n",
      "Epoch 149/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.6250 - val_loss: -164716.9375\n",
      "Epoch 150/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167586.0938 - val_loss: -164716.9375\n",
      "Epoch 151/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.5938 - val_loss: -164716.9531\n",
      "Epoch 152/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.9375 - val_loss: -164716.9531\n",
      "Epoch 153/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167589.0938 - val_loss: -164716.5625\n",
      "Epoch 154/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167595.4219 - val_loss: -164716.9531\n",
      "Epoch 155/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.7500 - val_loss: -164716.9531\n",
      "Epoch 156/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.0781 - val_loss: -164716.9531\n",
      "Epoch 157/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.4688 - val_loss: -164716.9531\n",
      "Epoch 158/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.4688 - val_loss: -164716.4219\n",
      "Epoch 159/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167583.0469 - val_loss: -164716.3906\n",
      "Epoch 160/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167595.3750 - val_loss: -164716.4062\n",
      "Epoch 161/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.2500 - val_loss: -164716.4219\n",
      "Epoch 162/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167584.1875 - val_loss: -164716.4375\n",
      "Epoch 163/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.2188 - val_loss: -164716.4844\n",
      "Epoch 164/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.8906 - val_loss: -164716.9531\n",
      "Epoch 165/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167581.0781 - val_loss: -164716.9531\n",
      "Epoch 166/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167588.9844 - val_loss: -164716.9531\n",
      "Epoch 167/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167595.6406 - val_loss: -164716.9531\n",
      "Epoch 168/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167583.4844 - val_loss: -30817.0254\n",
      "Epoch 169/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167591.8906 - val_loss: -31304.6484\n",
      "Epoch 170/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.0312 - val_loss: -164716.9531\n",
      "Epoch 171/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167580.7500 - val_loss: -164716.9531\n",
      "Epoch 172/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.2812 - val_loss: -164716.9531\n",
      "Epoch 173/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -166142.9688 - val_loss: -164716.9531\n",
      "Epoch 174/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -166902.9062 - val_loss: -164716.9375\n",
      "Epoch 175/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -166692.2656 - val_loss: -130339.8594\n",
      "Epoch 176/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167244.4062 - val_loss: -160926.5156\n",
      "Epoch 177/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167568.3438 - val_loss: -164716.9375\n",
      "Epoch 178/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167258.7969 - val_loss: -148074.6562\n",
      "Epoch 179/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167565.0000 - val_loss: -142450.3125\n",
      "Epoch 180/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167565.7188 - val_loss: -163806.2500\n",
      "Epoch 181/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167537.5156 - val_loss: -164716.9531\n",
      "Epoch 182/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167529.8281 - val_loss: -155314.1406\n",
      "Epoch 183/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167551.9688 - val_loss: -161493.7500\n",
      "Epoch 184/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167539.8438 - val_loss: -162524.9062\n",
      "Epoch 185/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167556.3906 - val_loss: -164716.9531\n",
      "Epoch 186/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167553.1250 - val_loss: -164170.2031\n",
      "Epoch 187/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167553.6250 - val_loss: -163351.4062\n",
      "Epoch 188/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167563.8281 - val_loss: -164647.4531\n",
      "Epoch 189/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167554.3438 - val_loss: -164177.1719\n",
      "Epoch 190/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167550.1250 - val_loss: -164330.3906\n",
      "Epoch 191/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167574.7656 - val_loss: -164256.6406\n",
      "Epoch 192/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167566.2812 - val_loss: -164679.9531\n",
      "Epoch 193/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167562.7656 - val_loss: -164679.9844\n",
      "Epoch 194/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167583.5000 - val_loss: -164680.1719\n",
      "Epoch 195/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167550.7656 - val_loss: -164679.8906\n",
      "Epoch 196/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167568.4844 - val_loss: -164680.9062\n",
      "Epoch 197/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167582.8750 - val_loss: -162427.0000\n",
      "Epoch 198/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167592.8125 - val_loss: -164681.0156\n",
      "Epoch 199/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167581.2656 - val_loss: -164691.8125\n",
      "Epoch 200/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167579.7500 - val_loss: -164716.9844\n",
      "Epoch 201/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167575.8281 - val_loss: -163970.7188\n",
      "Epoch 202/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167585.0469 - val_loss: -163598.8906\n",
      "Epoch 203/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167590.4844 - val_loss: -164692.4688\n",
      "Epoch 204/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167575.0000 - val_loss: -164693.2500\n",
      "Epoch 205/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.5312 - val_loss: -163677.4531\n",
      "Epoch 206/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167594.4844 - val_loss: -162334.8438\n",
      "Epoch 207/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167578.2812 - val_loss: -164698.7969\n",
      "Epoch 208/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167596.9688 - val_loss: -164699.4688\n",
      "Epoch 209/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167584.9375 - val_loss: -164698.7812\n",
      "Epoch 210/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167596.4375 - val_loss: -164700.7812\n",
      "Epoch 211/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167597.7031 - val_loss: -164699.4531\n",
      "Epoch 212/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167597.9062 - val_loss: -164705.6406\n",
      "Epoch 213/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167587.7812 - val_loss: -164706.6250\n",
      "Epoch 214/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167597.5469 - val_loss: -164613.1250\n",
      "Epoch 215/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167598.3750 - val_loss: -164712.5156\n",
      "Epoch 216/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167597.6406 - val_loss: -164708.0469\n",
      "Epoch 217/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9375 - val_loss: -164711.7812\n",
      "Epoch 218/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9062 - val_loss: -164603.3906\n",
      "Epoch 219/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.2031 - val_loss: -164625.2344\n",
      "Epoch 220/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9844 - val_loss: -164685.6875\n",
      "Epoch 221/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167597.2344 - val_loss: -164685.6406\n",
      "Epoch 222/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.0625 - val_loss: -164712.2812\n",
      "Epoch 223/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.3750 - val_loss: -164712.3906\n",
      "Epoch 224/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1562 - val_loss: -164711.9688\n",
      "Epoch 225/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167598.0469 - val_loss: -164716.9844\n",
      "Epoch 226/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167598.7188 - val_loss: -164716.9844\n",
      "Epoch 227/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167598.3594 - val_loss: -164716.9844\n",
      "Epoch 228/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.3281 - val_loss: -164698.5156\n",
      "Epoch 229/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.2031 - val_loss: -164716.9844\n",
      "Epoch 230/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167598.6094 - val_loss: -164698.5781\n",
      "Epoch 231/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0625 - val_loss: -164716.9844\n",
      "Epoch 232/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7656 - val_loss: -164716.9844\n",
      "Epoch 233/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.4219 - val_loss: -164716.9844\n",
      "Epoch 234/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.4375 - val_loss: -164716.7500\n",
      "Epoch 235/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6406 - val_loss: -164716.9844\n",
      "Epoch 236/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.1562 - val_loss: -164716.9844\n",
      "Epoch 237/1000\n",
      "113/113 [==============================] - 19s 172ms/step - loss: -167599.8750 - val_loss: -164651.2812\n",
      "Epoch 238/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167598.6875 - val_loss: -164690.2500\n",
      "Epoch 239/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.2031 - val_loss: -164676.3906\n",
      "Epoch 240/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8125 - val_loss: -164716.9844\n",
      "Epoch 241/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.2344 - val_loss: -164716.9844\n",
      "Epoch 242/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9531 - val_loss: -164716.9844\n",
      "Epoch 243/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8125 - val_loss: -164716.9844\n",
      "Epoch 244/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.3281 - val_loss: -164716.9844\n",
      "Epoch 245/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2031 - val_loss: -164689.4375\n",
      "Epoch 246/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.5156 - val_loss: -164716.9844\n",
      "Epoch 247/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2500 - val_loss: -164716.9844\n",
      "Epoch 248/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6250 - val_loss: -164716.5312\n",
      "Epoch 249/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7969 - val_loss: -164680.2031\n",
      "Epoch 250/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8125 - val_loss: -164679.0156\n",
      "Epoch 251/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7812 - val_loss: -164716.9844\n",
      "Epoch 252/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.5312 - val_loss: -164716.9844\n",
      "Epoch 253/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.4219 - val_loss: -164716.9844\n",
      "Epoch 254/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2188 - val_loss: -164716.9844\n",
      "Epoch 255/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9062 - val_loss: -164716.9844\n",
      "Epoch 256/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.2812 - val_loss: -164675.9844\n",
      "Epoch 257/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2812 - val_loss: -164716.9844\n",
      "Epoch 258/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7344 - val_loss: -164716.9844\n",
      "Epoch 259/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9375 - val_loss: -164716.9844\n",
      "Epoch 260/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7812 - val_loss: -164716.9844\n",
      "Epoch 261/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0625 - val_loss: -164716.9844\n",
      "Epoch 262/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6250 - val_loss: -164716.9844\n",
      "Epoch 263/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5312 - val_loss: -164716.9844\n",
      "Epoch 264/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.1094 - val_loss: -164687.2344\n",
      "Epoch 265/1000\n",
      "113/113 [==============================] - 19s 172ms/step - loss: -167600.0156 - val_loss: -164716.9844\n",
      "Epoch 266/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.2969 - val_loss: -164716.9844\n",
      "Epoch 267/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0469 - val_loss: -164691.0156\n",
      "Epoch 268/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3281 - val_loss: -164716.9844\n",
      "Epoch 269/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9531 - val_loss: -164716.9844\n",
      "Epoch 270/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8125 - val_loss: -164675.8281\n",
      "Epoch 271/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2500 - val_loss: -164716.9844\n",
      "Epoch 272/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0938 - val_loss: -164716.9844\n",
      "Epoch 273/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8750 - val_loss: -164716.9844\n",
      "Epoch 274/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7500 - val_loss: -164716.9844\n",
      "Epoch 275/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0625 - val_loss: -164716.9844\n",
      "Epoch 276/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0781 - val_loss: -164716.9844\n",
      "Epoch 277/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7812 - val_loss: -164716.9844\n",
      "Epoch 278/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2188 - val_loss: -164716.9844\n",
      "Epoch 279/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0469 - val_loss: -164716.9844\n",
      "Epoch 280/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6719 - val_loss: -164716.5781\n",
      "Epoch 281/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.1562 - val_loss: -164690.6406\n",
      "Epoch 282/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2656 - val_loss: -164716.9844\n",
      "Epoch 283/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8594 - val_loss: -164716.9844\n",
      "Epoch 284/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3906 - val_loss: -164716.9844\n",
      "Epoch 285/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7969 - val_loss: -164690.3750\n",
      "Epoch 286/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.7188 - val_loss: -164716.9844\n",
      "Epoch 287/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8125 - val_loss: -164716.9844\n",
      "Epoch 288/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0781 - val_loss: -164716.9844\n",
      "Epoch 289/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.4531 - val_loss: -164716.9844\n",
      "Epoch 290/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2969 - val_loss: -164716.9844\n",
      "Epoch 291/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0625 - val_loss: -164690.5156\n",
      "Epoch 292/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0781 - val_loss: -164716.9844\n",
      "Epoch 293/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0625 - val_loss: -164716.9844\n",
      "Epoch 294/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5469 - val_loss: -164716.9844\n",
      "Epoch 295/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8281 - val_loss: -164716.2188\n",
      "Epoch 296/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2500 - val_loss: -164716.7188\n",
      "Epoch 297/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7031 - val_loss: -164716.7344\n",
      "Epoch 298/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7812 - val_loss: -164716.9844\n",
      "Epoch 299/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1562 - val_loss: -164716.9844\n",
      "Epoch 300/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2969 - val_loss: -164716.9844\n",
      "Epoch 301/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2500 - val_loss: -164716.9844\n",
      "Epoch 302/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2969 - val_loss: -164716.9844\n",
      "Epoch 303/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0469 - val_loss: -164716.9844\n",
      "Epoch 304/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2500 - val_loss: -164716.9844\n",
      "Epoch 305/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2031 - val_loss: -164716.9844\n",
      "Epoch 306/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9531 - val_loss: -164716.9844\n",
      "Epoch 307/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0625 - val_loss: -164716.9844\n",
      "Epoch 308/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3750 - val_loss: -164716.9844\n",
      "Epoch 309/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1250 - val_loss: -164717.0000\n",
      "Epoch 310/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6250 - val_loss: -164716.9844\n",
      "Epoch 311/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7500 - val_loss: -164716.9844\n",
      "Epoch 312/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0781 - val_loss: -164717.0469\n",
      "Epoch 313/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7656 - val_loss: -164695.8281\n",
      "Epoch 314/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3750 - val_loss: -164716.7344\n",
      "Epoch 315/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0312 - val_loss: -164717.0000\n",
      "Epoch 316/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2656 - val_loss: -164716.9844\n",
      "Epoch 317/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2969 - val_loss: -164689.8594\n",
      "Epoch 318/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7031 - val_loss: -164717.0000\n",
      "Epoch 319/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5625 - val_loss: -164716.9844\n",
      "Epoch 320/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9375 - val_loss: -164716.9844\n",
      "Epoch 321/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7500 - val_loss: -164716.9844\n",
      "Epoch 322/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6875 - val_loss: -164716.3906\n",
      "Epoch 323/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9219 - val_loss: -164716.4062\n",
      "Epoch 324/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8750 - val_loss: -164716.4062\n",
      "Epoch 325/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9062 - val_loss: -164716.4219\n",
      "Epoch 326/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3750 - val_loss: -164716.4375\n",
      "Epoch 327/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.3281 - val_loss: -164688.5156\n",
      "Epoch 328/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1406 - val_loss: -164717.0312\n",
      "Epoch 329/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6875 - val_loss: -164712.5625\n",
      "Epoch 330/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9375 - val_loss: -164717.0000\n",
      "Epoch 331/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0625 - val_loss: -164717.0000\n",
      "Epoch 332/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.5625 - val_loss: -164716.9844\n",
      "Epoch 333/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7344 - val_loss: -164717.0312\n",
      "Epoch 334/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8750 - val_loss: -164716.9844\n",
      "Epoch 335/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.4062 - val_loss: -164716.9844\n",
      "Epoch 336/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.4531 - val_loss: -164716.1406\n",
      "Epoch 337/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1875 - val_loss: -164716.2188\n",
      "Epoch 338/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7344 - val_loss: -164716.7188\n",
      "Epoch 339/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6406 - val_loss: -164716.7344\n",
      "Epoch 340/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6875 - val_loss: -164716.8594\n",
      "Epoch 341/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3438 - val_loss: -164716.9844\n",
      "Epoch 342/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7969 - val_loss: -164717.0312\n",
      "Epoch 343/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.4375 - val_loss: -164717.0469\n",
      "Epoch 344/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3750 - val_loss: -164716.9844\n",
      "Epoch 345/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5000 - val_loss: -164715.2188\n",
      "Epoch 346/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.4688 - val_loss: -164716.9844\n",
      "Epoch 347/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3750 - val_loss: -164717.0000\n",
      "Epoch 348/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.2812 - val_loss: -164717.0000\n",
      "Epoch 349/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5156 - val_loss: -164717.0312\n",
      "Epoch 350/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8750 - val_loss: -164717.0000\n",
      "Epoch 351/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6562 - val_loss: -164717.0469\n",
      "Epoch 352/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.0312 - val_loss: -164717.0312\n",
      "Epoch 353/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5156 - val_loss: -164717.0000\n",
      "Epoch 354/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.7969 - val_loss: -164717.0000\n",
      "Epoch 355/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.4062 - val_loss: -164717.0312\n",
      "Epoch 356/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7656 - val_loss: -164717.0000\n",
      "Epoch 357/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2812 - val_loss: -164717.0469\n",
      "Epoch 358/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5781 - val_loss: -164717.0000\n",
      "Epoch 359/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.4219 - val_loss: -164717.0469\n",
      "Epoch 360/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6562 - val_loss: -164717.0312\n",
      "Epoch 361/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9219 - val_loss: -164716.1094\n",
      "Epoch 362/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0469 - val_loss: -164716.1875\n",
      "Epoch 363/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8906 - val_loss: -164716.2344\n",
      "Epoch 364/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.8906 - val_loss: -164716.8125\n",
      "Epoch 365/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.7031 - val_loss: -164716.4688\n",
      "Epoch 366/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2812 - val_loss: -164717.0469\n",
      "Epoch 367/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.9375 - val_loss: -164717.0000\n",
      "Epoch 368/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3438 - val_loss: -164717.0000\n",
      "Epoch 369/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3438 - val_loss: -164716.9844\n",
      "Epoch 370/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.7656 - val_loss: -164716.4062\n",
      "Epoch 371/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.2812 - val_loss: -164708.9688\n",
      "Epoch 372/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1562 - val_loss: -164716.4688\n",
      "Epoch 373/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1406 - val_loss: -164716.4844\n",
      "Epoch 374/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0312 - val_loss: -164711.5469\n",
      "Epoch 375/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.3750 - val_loss: -164717.0312\n",
      "Epoch 376/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.1250 - val_loss: -164717.0469\n",
      "Epoch 377/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.6250 - val_loss: -164717.0312\n",
      "Epoch 378/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0156 - val_loss: -164717.0312\n",
      "Epoch 379/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.1250 - val_loss: -164717.0312\n",
      "Epoch 380/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167601.0469 - val_loss: -164717.0469\n",
      "Epoch 381/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167599.6094 - val_loss: -164717.0469\n",
      "Epoch 382/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0938 - val_loss: -164717.0469\n",
      "Epoch 383/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.5000 - val_loss: -164716.5625\n",
      "Epoch 384/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0781 - val_loss: -164717.0469\n",
      "Epoch 385/1000\n",
      "113/113 [==============================] - 19s 171ms/step - loss: -167600.0469 - val_loss: -164717.0312\n",
      "Epoch 386/1000\n",
      " 34/113 [========>.....................] - ETA: 12s - loss: -167697.8281"
     ]
    }
   ],
   "source": [
    "model, embedder = build_model(\n",
    "    input_shape=X[0].shape, \n",
    "    output_shape=Y_sg[0].shape, \n",
    "    embed_size=8, \n",
    "    vocab_size=len(encode_dict)+1, \n",
    "    rnn_size=64\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")\n",
    "print(model.summary())\n",
    "\n",
    "train_hx = model.fit(\n",
    "    X, \n",
    "    Y_sg, \n",
    "    validation_split=0.2, \n",
    "    epochs=1000, \n",
    "    callbacks=[keras.callbacks.TensorBoard()], \n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1be35fa6cd7ba76b6b637ae5ebbc3292c3f6fe8b56e54002bc7d184d0bc4977"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
